{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19580,
     "status": "ok",
     "timestamp": 1750749896964,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "A0GKY0BsyP3-",
    "outputId": "c33c1807-484c-4152-a038-9e1c2722b2a4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import sys\n",
    "proj_dir = '/content/drive/MyDrive/fed_learning'\n",
    "sys.path.insert(0, proj_dir)\n",
    "os.chdir(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17074,
     "status": "ok",
     "timestamp": 1750749914060,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "yO_NGGPo1O7F",
    "outputId": "c771d873-7a32-4a70-a918-9364224b5513"
   },
   "outputs": [],
   "source": [
    "# Stage 1 – Centralized Training\n",
    "\n",
    "from data_utils import load_cifar100, split_data_iid, split_data_noniid\n",
    "from centralized_baseline import train_centralized_model, get_dataloaders\n",
    "from sparse_utils import compute_sensitivity, calibrate_gradient_mask, SparseSGD\n",
    "from fl_model_utils import evaluate_model, federated_aggregation\n",
    "from modelsaver import ModelSaver\n",
    "from dino_vits16 import load_dino_vits16, DINOClassifier\n",
    "from secure_aggregation import secure_aggregate\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12518,
     "status": "ok",
     "timestamp": 1750749926573,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "L4CNDAq71R9A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import gc\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load CIFAR-100\n",
    "train_ds, val_ds, test_ds = load_cifar100(validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "ec5f35839a9747afa0bffe4d6f6e1bc7",
      "255d8b1943d84ce2af4164559ac14271",
      "abfeb80b3d564fde843d116767f2b1e7",
      "23271175f0ee4df1a8718f65198997d4",
      "320936f226864b51a3df284635ce1dfe",
      "1fb2523d3c3a4ea88b86ddf4fff21230",
      "69b3a34cbbb4426bbfacab2a03313e27",
      "e3bb9f9378d948e690c54fc223c6f6eb",
      "3c915032339549918831ff6826df0a15",
      "dc0d1aeeaa5a401180393e2178c4a783",
      "2730628ac97d42fe8a24cd3eee343108"
     ]
    },
    "executionInfo": {
     "elapsed": 12291,
     "status": "ok",
     "timestamp": 1750749938892,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "eJXxUhbh19n7",
    "outputId": "6a50b85c-d043-463e-aec6-2e0106e6e526"
   },
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "# Centralized Training of DINO on CIFAR-100\n",
    "\n",
    "# Step 2: Load datasets (with val split)\n",
    "train_ds, val_ds, test_ds = load_cifar100(validation_split=0.1)\n",
    "train_loader, val_loader, test_loader = get_dataloaders(train_ds, val_ds, test_ds, batch_size=64)\n",
    "\n",
    "# Paths\n",
    "centralized_dir = \"/content/drive/MyDrive/fed_learning/centralized\"\n",
    "best_model_path = os.path.join(centralized_dir, \"best_model.pth\")\n",
    "final_model_path = os.path.join(centralized_dir, \"final_model.pth\")\n",
    "\n",
    "# Load DINO backbone + classifier\n",
    "backbone = load_dino_vits16()  # frozen ViT\n",
    "model = DINOClassifier(backbone, num_classes=100).to(device)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_centralized_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=25,\n",
    "    lr=0.01,\n",
    "    save_dir=centralized_dir\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    saver = ModelSaver(save_dir)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, val_accs, test_accs = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        for inputs, targets in loop:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        val_acc = evaluate_model(model, val_loader, device)\n",
    "        test_acc = evaluate_model(model, test_loader, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f\"📊 Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        saver.save_log(epoch+1, val_acc)\n",
    "        saver.save_best(model, val_acc)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    saver.save_final(model)\n",
    "    print(\"✅ Centralized training complete!\")\n",
    "\n",
    "\n",
    "# ====== Load or train centralized model ======\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    print(\"✅ Loaded existing trained centralized model. Skipping training.\")\n",
    "else:\n",
    "    train_centralized_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=25,\n",
    "        lr=0.01,\n",
    "        save_dir=centralized_dir\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5221,
     "status": "ok",
     "timestamp": 1750749944086,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "ZITByEVF5cGC",
    "outputId": "936c5577-3b37-47b3-b67a-0042d81b9a3d"
   },
   "outputs": [],
   "source": [
    "# STAGE 2 — Sparse Fine-Tuning\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ==== Config ====\n",
    "sparsity = 0.8\n",
    "mask_strategy = 'least-sensitive'\n",
    "fine_tune_epochs = 25\n",
    "lr = 0.01\n",
    "\n",
    "save_dir = '/content/drive/MyDrive/fed_learning/sparse_finetune'\n",
    "checkpoint_path = '/content/drive/MyDrive/fed_learning/centralized/best_model.pth'\n",
    "mask_path = os.path.join(save_dir, \"logs\", \"mask_tensor.pt\")\n",
    "best_sparse_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "final_sparse_model_path = os.path.join(save_dir, \"final_model.pth\")\n",
    "\n",
    "# ==== Load DINO model ====\n",
    "backbone = load_dino_vits16()\n",
    "model = DINOClassifier(backbone, num_classes=100).to(device)\n",
    "\n",
    "# ==== Skip if already exists ====\n",
    "if os.path.exists(best_sparse_model_path):\n",
    "    model.load_state_dict(torch.load(best_sparse_model_path, map_location=device))\n",
    "    print(\"✅ Sparse fine-tuned best model already exists. Skipping fine-tuning.\")\n",
    "else:\n",
    "    # Setup dirs\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(mask_path), exist_ok=True)\n",
    "\n",
    "    # Load centralized checkpoint\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    print(\"✅ Centralized checkpoint loaded.\")\n",
    "\n",
    "    # Setup optimizer/loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    model_saver = ModelSaver(save_dir)\n",
    "\n",
    "    # Load or compute mask\n",
    "    if os.path.exists(mask_path):\n",
    "        mask = torch.load(mask_path)\n",
    "        print(\"🎯 Mask loaded from disk.\")\n",
    "    else:\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            break  # single batch for sensitivity\n",
    "\n",
    "        sensitivity_info = compute_sensitivity(model)\n",
    "        mask = calibrate_gradient_mask(model, strategy=mask_strategy, sparsity=sparsity, fisher_info=sensitivity_info)\n",
    "        torch.save(mask, mask_path)\n",
    "        print(\"🎯 New mask computed and saved.\")\n",
    "\n",
    "    # Masking function\n",
    "    def apply_gradient_mask(model, mask):\n",
    "        with torch.no_grad():\n",
    "            for (name, param), mask_tensor in zip(model.named_parameters(), mask):\n",
    "                if param.grad is not None:\n",
    "                    param.grad *= mask_tensor.to(param.device)\n",
    "\n",
    "    # Sparse fine-tuning loop\n",
    "    for epoch in range(fine_tune_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            apply_gradient_mask(model, mask)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        acc = 100. * correct / total\n",
    "        loss_avg = running_loss / total\n",
    "        print(f\"📦 Epoch {epoch+1} | Loss: {loss_avg:.4f} | Accuracy: {acc:.2f}%\")\n",
    "\n",
    "        model_saver.save_log(epoch + 1, acc)\n",
    "        model_saver.save_best(model, acc)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    torch.save(model.state_dict(), final_sparse_model_path)\n",
    "    print(\"💾 Final sparse fine-tuned model saved.\")\n",
    "    print(\"✅ Sparse fine-tuning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1750749944091,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "uu8Lq5Tf5why"
   },
   "outputs": [],
   "source": [
    "#STAGE 3\n",
    "import time\n",
    "from copy import deepcopy\n",
    "def train_federated_sparse(model, client_datasets, val_loader, train_ds, device,\n",
    "                           rounds=25, clients_per_round=10, local_epochs=4, lr=0.01,\n",
    "                           sparsity=0.8, mask_strategy='least-sensitive',\n",
    "                           save_dir='/content/drive/MyDrive/fed_learning/fed_sparse_models'):\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    log_dir = os.path.join(save_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    saver = ModelSaver(save_dir)\n",
    "    global_model = deepcopy(model).to(device)\n",
    "    val_accs = []\n",
    "\n",
    "    # Compute or load sensitivity-based gradient mask\n",
    "    mask_path = os.path.join(log_dir, \"mask_tensor.pt\")\n",
    "    if os.path.exists(mask_path):\n",
    "        mask = torch.load(mask_path)\n",
    "        print(\"🎯 Loaded precomputed gradient mask.\")\n",
    "    else:\n",
    "        print(\"🎯 Computing gradient sensitivity for mask...\")\n",
    "        global_model.train()\n",
    "        optimizer = torch.optim.SGD(global_model.parameters(), lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "        inputs, targets = next(iter(loader))\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = global_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        sensitivity_info = compute_sensitivity(global_model)\n",
    "        mask = calibrate_gradient_mask(global_model, strategy=mask_strategy, sparsity=sparsity, fisher_info=sensitivity_info)\n",
    "        torch.save(mask, mask_path)\n",
    "        print(\"✅ Mask computed and saved.\")\n",
    "\n",
    "    def apply_gradient_mask(model, mask):\n",
    "        with torch.no_grad():\n",
    "            for (name, param), m in zip(model.named_parameters(), mask):\n",
    "                if param.grad is not None:\n",
    "                    param.grad *= m.to(param.device)\n",
    "\n",
    "    for round_num in range(rounds):\n",
    "        print(f\"\\n🔁 [Sparse FL] Round {round_num + 1}/{rounds}\")\n",
    "        selected_clients = random.sample(range(len(client_datasets)), clients_per_round)\n",
    "        client_models, client_weights = [], []\n",
    "\n",
    "        for client_idx in selected_clients:\n",
    "            client_model = deepcopy(global_model).to(device)\n",
    "            client_model.train()\n",
    "            optimizer = torch.optim.SGD(client_model.parameters(), lr=lr, momentum=0.9)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            loader = DataLoader(client_datasets[client_idx], batch_size=64, shuffle=True)\n",
    "\n",
    "            for _ in range(local_epochs):\n",
    "                for inputs, targets in loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = client_model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    apply_gradient_mask(client_model, mask)\n",
    "                    optimizer.step()\n",
    "\n",
    "            client_models.append(client_model.cpu().state_dict())\n",
    "            client_weights.append(len(client_datasets[client_idx]))\n",
    "\n",
    "        # Aggregate updated weights\n",
    "        global_model = federated_aggregation(client_models, client_weights, global_model)\n",
    "\n",
    "        # Evaluate and save\n",
    "        val_acc = evaluate_model(global_model, val_loader, device)\n",
    "        saver.save_log(round_num + 1, val_acc)\n",
    "        saver.save_best(global_model, val_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    saver.save_final(global_model)\n",
    "    torch.save(global_model.state_dict(), os.path.join(save_dir, \"model.pt\"))  # <-- Add this\n",
    "    return global_model, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10988,
     "status": "ok",
     "timestamp": 1750749955080,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "gyIYrYFC7GHU",
    "outputId": "3bb1c25c-8e18-48a9-b3ae-181d4177f2a2"
   },
   "outputs": [],
   "source": [
    "# STAGE 3: Federated Sparse Fine-Tuning\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "# Federated Learning Config\n",
    "K = 100  # Total number of clients\n",
    "C = 0.1  # Fraction of clients per round\n",
    "J = 4    # Local epochs per client\n",
    "num_rounds = 20  # Communication rounds\n",
    "lr = 0.01\n",
    "sparsity = 0.8\n",
    "mask_strategy = 'least-sensitive'\n",
    "save_dir = '/content/drive/MyDrive/fed_learning/fed_sparse_models'\n",
    "best_model_path = os.path.join(save_dir, 'best_model.pth')  # ✅ Define path correctly\n",
    "\n",
    "# ===== Data Loading =====\n",
    "train_ds, val_ds, test_ds = load_cifar100(validation_split=0.1)\n",
    "client_datasets = split_data_iid(train_ds, num_clients=K)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# ===== Load DINO backbone and classifier =====\n",
    "backbone = load_dino_vits16()\n",
    "model = DINOClassifier(backbone, num_classes=100).to(device)\n",
    "\n",
    "# ===== Load centralized checkpoint =====\n",
    "ckpt_path = '/content/drive/MyDrive/fed_learning/centralized/best_model.pth'\n",
    "central_ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(central_ckpt)\n",
    "print(\"✅ Loaded centralized model checkpoint.\")\n",
    "\n",
    "# ===== Check if sparse federated model already exists =====\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    print(\"✅ Loaded existing federated sparse fine-tuned model. Skipping training.\")\n",
    "\n",
    "else:\n",
    "    # ===== Run Federated Sparse Fine-Tuning =====\n",
    "    final_model, val_accs = train_federated_sparse(\n",
    "        model=deepcopy(model),\n",
    "        client_datasets=client_datasets,\n",
    "        val_loader=val_loader,\n",
    "        train_ds=train_ds,\n",
    "        device=device,\n",
    "        rounds=num_rounds,\n",
    "        clients_per_round=int(C * K),\n",
    "        local_epochs=J,\n",
    "        lr=lr,\n",
    "        sparsity=sparsity,\n",
    "        mask_strategy=mask_strategy,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "    print(\"💾 Federated sparse fine-tuning complete. Logs and checkpoints saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1750749955130,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "wRuqQCMq9XiF"
   },
   "outputs": [],
   "source": [
    "# Baseline Federated Training (FedAvg without Sparse Masking)\n",
    "\n",
    "def train_federated_baseline(model, client_datasets, val_loader, device,\n",
    "                             rounds=10, clients_per_round=10,\n",
    "                             local_epochs=1, lr=0.01,\n",
    "                             save_dir='/content/drive/MyDrive/fed_learning/fed_baseline_models'):\n",
    "\n",
    "    global_model = deepcopy(model).to(device)\n",
    "    saver = ModelSaver(save_dir)\n",
    "\n",
    "    for round_num in range(rounds):\n",
    "        print(f\"\\n🔁 [Baseline] Round {round_num + 1}/{rounds}\")\n",
    "        selected_clients = random.sample(range(len(client_datasets)), clients_per_round)\n",
    "\n",
    "        client_models = []\n",
    "        client_weights = []\n",
    "\n",
    "        for client_idx in selected_clients:\n",
    "            client_model = deepcopy(global_model).to(device)\n",
    "            client_model.train()\n",
    "            optimizer = torch.optim.SGD(client_model.parameters(), lr=lr, momentum=0.9)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loader = DataLoader(client_datasets[client_idx], batch_size=64, shuffle=True)\n",
    "\n",
    "            for epoch in range(local_epochs):\n",
    "                for inputs, targets in loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = client_model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            client_models.append(client_model.cpu().state_dict())\n",
    "            client_weights.append(len(client_datasets[client_idx]))\n",
    "\n",
    "            client_model.to(\"cpu\")\n",
    "            del client_model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        global_model = federated_aggregation(client_models, client_weights, global_model)\n",
    "\n",
    "        val_acc = evaluate_model(global_model, val_loader, device)\n",
    "        saver.save_log(round_num + 1, val_acc)\n",
    "        saver.save_best(global_model, val_acc)\n",
    "\n",
    "    saver.save_final(global_model)\n",
    "    torch.save(global_model.state_dict(), os.path.join(save_dir, \"model.pt\"))  # <-- Add this\n",
    "    print(\"✅ Baseline Federated Training complete.\")\n",
    "    return global_model, val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1750749955256,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "K9o29Lng9m8I",
    "outputId": "c4f389b4-2f1d-4066-8d75-c1e51fe40115"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "K = 100\n",
    "C = 0.1\n",
    "J = 4\n",
    "clients_per_round = int(K * C)\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    print(\"✅ Loaded existing fedrated baseline on iid. Skipping training.\")\n",
    "\n",
    "else:\n",
    "# IID\n",
    "  iid_clients = split_data_iid(train_ds, num_clients=K)\n",
    "  fed_model_iid, accs_iid = train_federated_baseline(\n",
    "      model=deepcopy(model),\n",
    "      client_datasets=iid_clients,\n",
    "      val_loader=val_loader,\n",
    "      device=device,\n",
    "      rounds=20,\n",
    "      clients_per_round=clients_per_round,\n",
    "      local_epochs=J,\n",
    "      lr=0.01,\n",
    "      save_dir='/content/drive/MyDrive/fed_learning/fed_baseline/iid'\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1750749955405,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "munF7-fC1Jdr"
   },
   "outputs": [],
   "source": [
    "def is_model_saved(save_dir):\n",
    "    model_path = os.path.join(save_dir, \"model.pt\")\n",
    "    print(f\"🔍 Checking for model at {model_path} — Exists? {os.path.exists(model_path)}\")\n",
    "    return os.path.exists(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1750749955746,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "UOFmYXkf2D75",
    "outputId": "3a65e0e5-edbf-4d39-ad1f-f2cc0916bdd6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.exists(\"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc1/model.pt\"))\n",
    "print(os.path.exists(\"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc1/model.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750749955759,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "SYk94fNUYJp3"
   },
   "outputs": [],
   "source": [
    "def run_federated_experiments_non_iid(model, full_dataset, train_ds, val_loader, device,\n",
    "                                      Nc=5, rounds=20, clients_per_round=10, local_epochs=4):\n",
    "    print(f\"\\n🔧 Creating non-IID split with Nc = {Nc}\")\n",
    "\n",
    "    client_indices = split_data_noniid(full_dataset, num_clients=100, num_classes_per_client=Nc)\n",
    "    train_ds_indices = set(train_ds.indices)\n",
    "\n",
    "    non_iid_clients = [\n",
    "        torch.utils.data.Subset(train_ds.dataset, [i for i in idxs if i in train_ds_indices])\n",
    "        for idxs in client_indices.values()\n",
    "    ]\n",
    "\n",
    "    baseline_dir = f\"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc{Nc}\"\n",
    "    sparse_dir   = f\"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc{Nc}\"\n",
    "\n",
    "    # Train FedAvg (only if not already saved)\n",
    "    if not is_model_saved(baseline_dir):\n",
    "        print(f\"🚀 Training FedAvg model for Nc = {Nc}\")\n",
    "        _ = train_federated_baseline(\n",
    "            model=deepcopy(model),\n",
    "            client_datasets=non_iid_clients,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            rounds=rounds,\n",
    "            clients_per_round=clients_per_round,\n",
    "            local_epochs=local_epochs,\n",
    "            lr=0.01,\n",
    "            save_dir=baseline_dir\n",
    "        )\n",
    "    else:\n",
    "        print(f\"✅ FedAvg model already exists for Nc = {Nc} — skipping.\")\n",
    "\n",
    "    # Train Sparse FedAvg (only if not already saved)\n",
    "    if not is_model_saved(sparse_dir):\n",
    "        print(f\"🚀 Training Sparse FedAvg model for Nc = {Nc}\")\n",
    "        _ = train_federated_sparse(\n",
    "            model=deepcopy(model),\n",
    "            client_datasets=non_iid_clients,\n",
    "            val_loader=val_loader,\n",
    "            train_ds=train_ds,\n",
    "            device=device,\n",
    "            rounds=rounds,\n",
    "            clients_per_round=clients_per_round,\n",
    "            local_epochs=local_epochs,\n",
    "            lr=0.01,\n",
    "            sparsity=0.8,\n",
    "            mask_strategy='least-sensitive',\n",
    "            save_dir=sparse_dir\n",
    "        )\n",
    "    else:\n",
    "        print(f\"✅ Sparse FedAvg model already exists for Nc = {Nc} — skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3727,
     "status": "ok",
     "timestamp": 1750749959490,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "2sdWOSHtoddj",
    "outputId": "fba5fca3-2eff-4d9f-c4da-4c340250b0c3"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "full_train_ds = CIFAR100(root='./data', train=True, download=True,\n",
    "                         transform=transforms.ToTensor())\n",
    "\n",
    "for Nc in [1, 5, 10, 50]:\n",
    "    run_federated_experiments_non_iid(model, full_train_ds, train_ds, val_loader, device,\n",
    "                                      Nc=Nc, rounds=20, clients_per_round=10, local_epochs=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451645,
     "status": "ok",
     "timestamp": 1750750411117,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "m78XDilKz8Je",
    "outputId": "e6a24858-bedd-448d-d25c-4a990df42093"
   },
   "outputs": [],
   "source": [
    "#Evaluation of all trained models on the same test set\n",
    "import os\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "# --- Final test function ---\n",
    "def final_test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100. * correct / total\n",
    "\n",
    "# --- ModelSaver with export_log ---\n",
    "class ModelSaver:\n",
    "    def __init__(self, save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.save_dir = save_dir\n",
    "        self.best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "        self.final_model_path = os.path.join(save_dir, 'final_model.pth')\n",
    "        self.log_path = os.path.join(save_dir, 'log.txt')\n",
    "\n",
    "    def save_log(self, key, value):\n",
    "        with open(self.log_path, 'a') as f:\n",
    "            f.write(f\"{key},{value:.4f}\\n\")\n",
    "\n",
    "    def export_log(self, output_csv_name=\"final_test_results.csv\"):\n",
    "        csv_path = os.path.join(self.save_dir, output_csv_name)\n",
    "        entries = []\n",
    "        if os.path.exists(self.log_path):\n",
    "            with open(self.log_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(',')\n",
    "                    if len(parts) == 2:\n",
    "                        key = parts[0].replace(\"_final_test_accuracy\", \"\")\n",
    "                        value = parts[1]\n",
    "                        entries.append((key, value))\n",
    "        if entries:\n",
    "            with open(csv_path, 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(['model_name', 'final_test_accuracy'])\n",
    "                writer.writerows(entries)\n",
    "            print(f\"📊 Final test results exported to: {csv_path}\")\n",
    "        else:\n",
    "            print(\"⚠️ No entries found in log file.\")\n",
    "\n",
    "# --- Evaluation function ---\n",
    "def load_and_evaluate(model_name, model_path, test_loader, device, saver):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"[{model_name}] ❌ Model not found at {model_path}\")\n",
    "        return\n",
    "    print(f\"[{model_name}] ✅ Evaluating...\")\n",
    "\n",
    "    model = DINOClassifier(load_dino_vits16(), num_classes=100).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    acc = final_test(model, test_loader, device)\n",
    "    print(f\"[{model_name}] Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # Log and save\n",
    "    saver.save_log(f\"{model_name}_final_test_accuracy\", acc)\n",
    "\n",
    "# --- Run evaluation ---\n",
    "final_results_saver = ModelSaver(\"/content/drive/MyDrive/fed_learning/final_test_results\")\n",
    "\n",
    "# Clear old log if you want to start fresh\n",
    "if os.path.exists(final_results_saver.log_path):\n",
    "    os.remove(final_results_saver.log_path)\n",
    "\n",
    "# Paths to saved models\n",
    "model_paths = {\n",
    "    \"centralized\": \"/content/drive/MyDrive/fed_learning/centralized/best_model.pth\",\n",
    "    \"sparse_finetuned\": \"/content/drive/MyDrive/fed_learning/sparse_finetune/best_model.pth\",\n",
    "\n",
    "    \"fed_sparse\": \"/content/drive/MyDrive/fed_learning/fed_sparse_models/best_model.pth\",\n",
    "    \"fed_sparse_noniid_nc1\": \"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc1/best_model.pth\",\n",
    "    \"fed_sparse_noniid_nc5\": \"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc5/best_model.pth\",\n",
    "    \"fed_sparse_noniid_nc10\": \"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc10/best_model.pth\",\n",
    "    \"fed_sparse_noniid_nc50\": \"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc50/best_model.pth\",\n",
    "\n",
    "    \"fed_baseline_iid\": \"/content/drive/MyDrive/fed_learning/fed_baseline/iid/best_model.pth\",\n",
    "    \"fed_baseline_noniid_nc1\": \"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc1/best_model.pth\",\n",
    "    \"fed_baseline_noniid_nc5\": \"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc5/best_model.pth\",\n",
    "    \"fed_baseline_noniid_nc10\": \"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc10/best_model.pth\",\n",
    "    \"fed_baseline_noniid_nc50\": \"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc50/best_model.pth\",\n",
    "}\n",
    "\n",
    "# Evaluate all\n",
    "for model_name, model_path in model_paths.items():\n",
    "    load_and_evaluate(model_name, model_path, test_loader, device, final_results_saver)\n",
    "\n",
    "# Export results\n",
    "final_results_saver.export_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1750750411133,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "9Uc3nTewmuQM"
   },
   "outputs": [],
   "source": [
    "#personal contribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1750750411520,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "QRtVIkXoF98d",
    "outputId": "de736858-baa5-40fc-f181-40217a838d86"
   },
   "outputs": [],
   "source": [
    "log_path = '/content/drive/MyDrive/fed_learning/fed_sparse_models/log.txt'\n",
    "rounds=[]\n",
    "accs=[]\n",
    "\n",
    "# 4)read and parse\n",
    "with open(log_path, 'r') as f:\n",
    "    for line in f:\n",
    "        m = re.match(r'Round\\s+(\\d+)\\s*\\|\\s*Val Acc:\\s*([\\d\\.]+)', line)\n",
    "        if m:\n",
    "            rounds.append(int(m.group(1)))\n",
    "            accs.append(float(m.group(2)))\n",
    "print(\"✅ Validation Accuracies:\", accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 748,
     "status": "ok",
     "timestamp": 1750750412265,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "YpFj6gSmXJB0",
    "outputId": "13577d45-350b-492c-8d5f-b795d517bf9b"
   },
   "outputs": [],
   "source": [
    "#select randomly 2 clients\n",
    "#and clone the model and 1 mini-step of SGD on every client to simulate an update.\n",
    "#collects in client_states relatives state_dict and in client_ids the ids of the clients\n",
    "selected = random.sample(list(client_datasets.keys()), 2)\n",
    "print(\"📋 selected Clients for the demo:\", selected)\n",
    "\n",
    "client_states = []\n",
    "client_ids    = []\n",
    "\n",
    "for cid in selected:\n",
    "    ds = client_datasets[cid]\n",
    "    loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "    cm = deepcopy(model).to(device)\n",
    "    cm.train()\n",
    "    optimizer = torch.optim.SGD(cm.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 1  mini-step to simulate an update\n",
    "    inputs, targets = next(iter(loader))\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out = cm(inputs)\n",
    "    loss = criterion(out, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # save the state_dict and ID\n",
    "    client_states.append(cm.cpu().state_dict())\n",
    "    client_ids.append(cid)\n",
    "\n",
    "    # free the memory\n",
    "    cm.to('cpu'); del cm; torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✅ Collected simulate state_dict for the clients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 23432,
     "status": "ok",
     "timestamp": 1750750435650,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "mWoGOFb4XVyX",
    "outputId": "84f8ac62-5a25-47c4-e0fe-79c211f39886"
   },
   "outputs": [],
   "source": [
    "#call secure_aggregate (clinet_states, client_ids) to generate an agg_state where zero-sum noise will delete\n",
    "#upload agg_state in sec_model, valutate val_loader and print the validation accuracy\n",
    "#take fed_final_acc = accs[-1] (the last accuracy of sparse FedAvg) and show in a bar chart that compare the 2 strategies\n",
    "\n",
    "agg_state = secure_aggregate(client_states, client_ids)\n",
    "print(\"🔐 secure_aggregate produced a state_dict aggregate.\")\n",
    "\n",
    "# Carico nello stesso modello finale e valuto\n",
    "sec_model = deepcopy(model).to(device)\n",
    "sec_model.load_state_dict(agg_state)\n",
    "sec_model.eval()\n",
    "\n",
    "sec_acc = evaluate_model(sec_model, val_loader, device)\n",
    "print(f\"📊 Demo SecureAgg Val Acc: {sec_acc:.4f}\")\n",
    "\n",
    "# 3) Confronto a barre con l’ultima fed_accs[-1]\n",
    "fed_final_acc = accs[-1]\n",
    "print(f\"📊 FedAvg final Val Accuracy: {fed_final_acc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "labels = ['FedAvg', 'SecureAgg']\n",
    "scores = [fed_final_acc, sec_acc]\n",
    "bars = plt.bar(labels, scores, color=['skyblue','salmon'])\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('FedAvg vs Secure Aggregation (Demo)')\n",
    "for bar, val in zip(bars, scores):\n",
    "    plt.text(bar.get_x()+bar.get_width()/2, val+0.005, f\"{val:.3f}\", ha='center')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 1600,
     "status": "ok",
     "timestamp": 1750750437253,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "UJNqh5vdajRe",
    "outputId": "e1cd883c-3894-4aaf-c529-c13de20ed7ed"
   },
   "outputs": [],
   "source": [
    "#PLOTS\n",
    "\n",
    "def plot_logs_percent(log_paths, labels, title=\"Validation Accuracy\", ylabel=\"Accuracy (%)\", save_path=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for log_path, label in zip(log_paths, labels):\n",
    "        if not os.path.exists(log_path):\n",
    "            print(f\"❌ Log file not found for {label}: {log_path}\")\n",
    "            continue\n",
    "\n",
    "        rounds, accs = [], []\n",
    "        with open(log_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                # Expect \"Round X | Val Acc: Y.YYYY\"\n",
    "                if \"| Val Acc:\" in line:\n",
    "                    left, right = line.split(\"| Val Acc:\")\n",
    "                    try:\n",
    "                        r = int(left.replace(\"Round\", \"\").strip())\n",
    "                        v = float(right.strip())\n",
    "                    except:\n",
    "                        # fallback to plain float\n",
    "                        continue\n",
    "                else:\n",
    "                    # maybe it's just a number per line\n",
    "                    try:\n",
    "                        r = len(rounds) + 1\n",
    "                        v = float(line)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                # if v is a fraction ∈[0,1], scale to percent\n",
    "                if 0 <= v <= 1:\n",
    "                    v *= 100.0\n",
    "\n",
    "                rounds.append(r)\n",
    "                accs.append(v)\n",
    "\n",
    "        if not rounds:\n",
    "            print(f\"⚠️ No valid entries in {log_path}\")\n",
    "            continue\n",
    "\n",
    "        plt.plot(rounds, accs, label=label)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch / Round\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"✅ Plot saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "os.makedirs(\"/content/drive/MyDrive/fed_learning/plots/\", exist_ok=True)\n",
    "\n",
    "# usage:\n",
    "plot_logs_percent(\n",
    "    log_paths=[\n",
    "        \"/content/drive/MyDrive/fed_learning/centralized/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/sparse_finetune/log.txt\"\n",
    "    ],\n",
    "    labels=[\"Centralized\", \"Sparse Fine-Tuning\"],\n",
    "    title=\"Centralized vs Sparse Fine-Tuning Accuracy\",\n",
    "    save_path=\"/content/drive/MyDrive/fed_learning/plots/centralized_vs_sparse.png\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 1560,
     "status": "ok",
     "timestamp": 1750750438819,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "RAEJcNp-m8_j",
    "outputId": "2297c49f-bafe-4c21-d917-905718a27ef5"
   },
   "outputs": [],
   "source": [
    "# Assuming the logs are located at \"<folder>/log.txt\", we build full paths\n",
    "plot_logs_percent(\n",
    "    log_paths=[\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_sparse_models/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_baseline/iid/log.txt\"\n",
    "    ],\n",
    "    labels=[\"Federated Sparse\", \"Federated Baseline (IID)\"],\n",
    "    title=\"Federated Training Comparison (IID)\",\n",
    "    save_path=\"/content/drive/MyDrive/fed_learning/plots/federatedtraining_comparison.png\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750750438824,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "5yKFvTWwnEHL"
   },
   "outputs": [],
   "source": [
    "def save_plot_logs(log_dirs, labels, save_path, title=\"Validation Accuracy\", ylabel=\"Accuracy (%)\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for log_dir, label in zip(log_dirs, labels):\n",
    "        log_path = os.path.join(log_dir, \"logs\", \"log.txt\")\n",
    "        if os.path.exists(log_path):\n",
    "            with open(log_path, \"r\") as f:\n",
    "                accs = [float(line.strip()) for line in f.readlines()]\n",
    "                plt.plot(range(1, len(accs) + 1), accs, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch / Round\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"✅ Plot saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1750750439638,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "w1xgm_GAnG3z",
    "outputId": "6893c00e-bf2f-4001-d9ad-ab6a6f5da75a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(metric_logs, labels, title=\"Training Metrics\", save_path=None):\n",
    "    \"\"\"\n",
    "    metric_logs: dict of {metric_name: list of log file paths}\n",
    "    labels: list of experiment names (same length as each metric's log list)\n",
    "    Example:\n",
    "    metric_logs = {\n",
    "        \"Validation Accuracy\": [path1, path2],\n",
    "        \"Training Loss\": [path3, path4],\n",
    "        \"Test Accuracy\": [path5, path6]\n",
    "    }\n",
    "    labels = [\"Experiment 1\", \"Experiment 2\"]\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = list(metric_logs.keys())\n",
    "    n_metrics = len(metrics)\n",
    "\n",
    "    fig, axs = plt.subplots(n_metrics, 1, figsize=(12, 5 * n_metrics))\n",
    "\n",
    "    if n_metrics == 1:\n",
    "        axs = [axs]  # make it iterable\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axs[i]\n",
    "        log_paths = metric_logs[metric]\n",
    "\n",
    "        for log_path, label in zip(log_paths, labels):\n",
    "            if os.path.exists(log_path):\n",
    "                with open(log_path, \"r\") as f:\n",
    "                    values = []\n",
    "                    for line in f.readlines():\n",
    "                        try:\n",
    "                            # Try to parse lines like \"Round X | Val Acc: Y.YYYY\"\n",
    "                            parts = line.strip().split(\" | \")\n",
    "                            if len(parts) == 2:\n",
    "                                # Extract value from format: \"Val Acc: 0.6512\"\n",
    "                                val_part = parts[1]\n",
    "                                value = float(val_part.split(\": \")[1])\n",
    "                                values.append(value)\n",
    "                            else:\n",
    "                                # fallback: just convert line to float\n",
    "                                values.append(float(line.strip()))\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "\n",
    "                    if values:\n",
    "                        ax.plot(range(1, len(values) + 1), values, label=label)\n",
    "                    else:\n",
    "                        print(f\"⚠️ No valid {metric} data found in {log_path}\")\n",
    "            else:\n",
    "                print(f\"❌ Log file not found for {label} - {metric}: {log_path}\")\n",
    "\n",
    "        ax.set_title(f\"{metric} Over Training\")\n",
    "        ax.set_xlabel(\"Epoch / Round\")\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (replace paths with your actual log file paths)\n",
    "plot_metrics(\n",
    "    metric_logs={\n",
    "        \"Validation Accuracy\": [\n",
    "            \"/content/drive/MyDrive/fed_learning/fed_sparse_models/log.txt\",\n",
    "            \"/content/drive/MyDrive/fed_learning/fed_baseline/iid/log.txt\"\n",
    "        ],\n",
    "        \"Training Loss\": [\n",
    "            \"/content/drive/MyDrive/fed_learning/fed_sparse_models/log.txt\",\n",
    "            \"/content/drive/MyDrive/fed_learning/fed_baseline/iid/log.txt\"\n",
    "        ],\n",
    "        \"Test Accuracy\": [\n",
    "            \"/content/drive/MyDrive/fed_learning/fed_sparse_models/log.txt\",\n",
    "            \"/content/drive/MyDrive/fed_learning/fed_baseline/iid/log.txt\"\n",
    "        ]\n",
    "    },\n",
    "    labels=[\"Federated Sparse\", \"Federated Baseline (IID)\"],\n",
    "    title=\" Federated Training Metrics Comparison (IID)\",\n",
    "    save_path=\"/content/drive/MyDrive/fed_learning/plots/fed_metric_comparison_iid.png\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1750752078469,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "LOadpNGMea33",
    "outputId": "d2022402-6f63-4b0e-c97c-897f2601e307"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_final_accuracy_noniid(nc_values, fedavg_accs, sparse_accs, title, save_path):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Line plot with markers\n",
    "    plt.plot(nc_values, fedavg_accs, marker='o', linestyle='-', label='FedAvg', color='tab:blue')\n",
    "    plt.plot(nc_values, sparse_accs, marker='s', linestyle='--', label='Sparse FedAvg', color='tab:orange')\n",
    "\n",
    "    plt.xlabel(\"Number of Classes per Client (Nc)\")\n",
    "    plt.ylabel(\"Final Test Accuracy (%)\")\n",
    "    plt.title(title, pad=20)\n",
    "    plt.xticks(nc_values)\n",
    "    plt.ylim(min(fedavg_accs + sparse_accs) - 2, max(fedavg_accs + sparse_accs) + 2)  # dynamic range with buffer\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Ensure layout doesn't cut title or labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight')  # ensures nothing gets cut off\n",
    "    plt.show()\n",
    "\n",
    "# Fixed accuracy values and call\n",
    "plot_final_accuracy_noniid(\n",
    "    nc_values=[1, 5, 10, 50],\n",
    "    fedavg_accs=[70.31, 63.41, 70.75, 72.54],   # Fixed typo 7075 → 70.75\n",
    "    sparse_accs=[67.02, 70.38, 71.57, 73.08],\n",
    "    title=\"Final Test Accuracy vs. Nc (Non-IID Data)\",\n",
    "    save_path=\"/content/drive/MyDrive/fed_learning/plots/noniid_final_accuracy_comparison.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 5017,
     "status": "ok",
     "timestamp": 1750751580074,
     "user": {
      "displayName": "Ajla",
      "userId": "08490805813754881498"
     },
     "user_tz": -120
    },
    "id": "utF59Ry76bQR",
    "outputId": "5eb7878e-5061-4cdd-f9db-2af1743f2650"
   },
   "outputs": [],
   "source": [
    "plot_logs_percent(\n",
    "    log_paths=[\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc1/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc5/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc10/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_baseline/noniid_nc50/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc1/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc5/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc10/log.txt\",\n",
    "        \"/content/drive/MyDrive/fed_learning/fed_sparse_models/sparse_noniid_nc50/log.txt\"\n",
    "\n",
    "    ],\n",
    "    labels=[\n",
    "        \"FedAvg (Nc=1)\", \"FedAvg (Nc=5)\", \"FedAvg (Nc=10)\", \"FedAvg (Nc=50)\",\n",
    "        \"Sparse (Nc=1)\", \"Sparse (Nc=5)\", \"Sparse (Nc=10)\", \"Sparse (Nc=50)\"\n",
    "    ],\n",
    "    title=\"Non-IID Federated Learning Comparison \",\n",
    "    save_path=\"/content/drive/MyDrive/fed_learning/plots/federatedtraining_noniid_comparison.png\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
